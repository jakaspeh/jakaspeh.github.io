---
layout: post
title:  "OpenMP: For"
date:   2016-05-02
categories: concurrency
---

The for loop construct is probably one of the most widely used features of the
OpenMP. The construct's aim is to parallelize an explicitly written for loop.

For loop construct
------------------

The syntax of the loop construct is 

{% highlight c++ %}
#pragma omp parallel [clauses]
{
    #pragma omp for [clauses]
    for (...)
    {
        // body 
    }
}
{% endhighlight %}

The `{%raw%}parallel{%endraw%}` construct specifies the region which should be
executed in parallel. A program without the parallel construct will be executed
sequentially.

The `{%raw%}for{%endraw%}` loop construct (or simply the loop construct)
specifies that the iterations of the following for loop will be executed in
parallel. The iterations are distributed across the threads that already exist.

If there is only one `{%raw%}#pragma omp for{%endraw%}` inside `{%raw%}#pragma
omp parallel{%endraw%}` we can simplify both constructs into the combined
construct.

{% highlight c++ %}
#pragma omp parallel for [clauses]
for (...)
{

}
{% endhighlight %}

Shared clause
-------------

The `{%raw%}clauses{%endraw%}` are additional options which we can set to the
constructs. 

An example of a clause for `{%raw%}parallel{%endraw%}` construct is
`{%raw%}shared(...){%endraw%}` clause. When a program encounters the
`{%raw%}parallel{%endraw%}` construct, the program forks a team of threads. The
variables, which are listed in the `{%raw%}shared(...){%endraw%}` clause, are
then shared between all the threads.

Example
-------

Let us write a first parallel loop with the OpenMP loop construct.  

{% highlight c++ %}
std::vector<int> iterations(omp_get_max_threads(), 0);
int n = 100;

#pragma omp parallel shared(iterations, n)
{    
    #pragma omp for 
    for (int i = 0; i != n; i++)
    {
        iterations[omp_get_thread_num()]++;
    }
}

print_iterations(iterations);
{% endhighlight %}

We define the vector of ints. Each thread increments its corresponding
entry in the vector. At the end, the `{%raw%}i{%endraw%}`-th entry of the vector
tells us how many iterations was executed by the `{%raw%}i{%endraw%}`-th thread.

The `{%raw%}parallel{%endraw%}` construct creates a team of threads which
execute in parallel. The variables `{%raw%}iterations{%endraw%}` and
`{%raw%}n{%endraw%}` are shared between all the threads. 

The loop construct specifies that the `{%raw%}for{%endraw%}` loop should be
executed in parallel.

We use a couple of OpenMP functions. The
`{%raw%}omp_get_max_threads(){%endraw%}` returns an upper bound on the number of
threads that could form a new team of threads. This upper bound is valid only if
we later do not explicitly specify the number of threads in the
team. Additionally, we also used `{%raw%}omp_get_thread_num(){%endraw%}` which
returns the number of the calling thread.

Canonical loop form
-------------------

When we compile the upper program, the compilation fails with an error. The error is 

{% highlight bash %}
In function ‘int main()’:
error: invalid controlling predicate
         for (int i = 0; i != n; i++)
{% endhighlight %}

What is wrong with our program? The printings suggests that the condition of the
for loop is invalid.

Well, OpenMP is able to parallelize a loop only if it has a certain
structure. The structure is

{% highlight c++ %}
for (initialize; test; increment)
{
...
}
{% endhighlight %}

The `{%raw%}initialize{%endraw%}` expression is  of the form `{%raw%}var =
lb{%endraw%}`, where `{%raw%}var{%endraw%}` is an integer or a random access
iterator and `{%raw%}lb{%endraw%}` is a loop invariant. 

The `{%raw%}test{%endraw%}` expression must have the form `{%raw%}var operator
b{%endraw%}` or `{%raw%}b operator var{%endraw%}`, where `{%raw%}b{%endraw%}` is
the loop invariant and `{%raw%}operator{%endraw%}` is one of the following

* `{%raw%}<{%endraw%}`,
* `{%raw%}<={%endraw%}`,
* `{%raw%}>{%endraw%}`,
* `{%raw%}>={%endraw%}`.

The `{%raw%}increment{%endraw%}` expression has to be one of the following 

* `{%raw%}++var{%endraw%}`,
* `{%raw%}var++{%endraw%}`,
* `{%raw%}--var{%endraw%}`,
* `{%raw%}var--{%endraw%}`,
* `{%raw%}var += incr{%endraw%}`,
* `{%raw%}var -= incr{%endraw%}`,
* `{%raw%}var = var + incr{%endraw%}`,
* `{%raw%}var = incr + var{%endraw%}`,
* `{%raw%}var = var - incr{%endraw%}`,

where `{%raw%}incr{%endraw%}` is a loop invariant integer expression. 

The loop, which satisfies these conditions, has the canonical loop form (defined
in the OpenMP specification). You can find the conditions and the precise
definition of the canonical loop form in the [OpenMP
specification](http://www.openmp.org/mp-documents/openmp-4.5.pdf) on the page
53.

Corrections
-----------

Having the knowledge about the canonical loop form, we can correct the test in
the for loop. The program now looks like

{% highlight c++ %}
std::vector<int> iterations(omp_get_max_threads(), 0);
int n = 100;

#pragma omp parallel shared(iterations, n)
{    
    #pragma omp for 
    for (int i = 0; i < n; i++)
    {
        iterations[omp_get_thread_num()]++;
    }
}

print_iterations(iterations);
{% endhighlight %}

The whole source code is available [here](https://github.com/jakaspeh/concurrency/blob/master/ompFor.cpp).

The output of the program is 

{% highlight c++ %}
$ ./ompFor
Thread 0 -> 13 iterations
Thread 1 -> 13 iterations
Thread 2 -> 13 iterations
Thread 3 -> 13 iterations
Thread 4 -> 12 iterations
Thread 5 -> 12 iterations
Thread 6 -> 12 iterations
Thread 7 -> 12 iterations
{% endhighlight %}

The program used eight threads to execute the loop. OpenMP uniformly divided the
iterations between all the threads -- each one executed 12 or 13 iterations. 

Summary
-------

In this article, we looked at the basics of the OpenMP loop construct. In order to
parallelize the for loop, it must be in the canonical loop form. We examined the
definition of the canonical loop form. At the end, we parallelized a for loop
with the OpenMP loop clause.

Links:

* [Source code](https://github.com/jakaspeh/concurrency/blob/master/ompFor.cpp)
* [Jaka's Corner: Introduction to OpenMP](/blog/2016/04/omp-introduction.html)


